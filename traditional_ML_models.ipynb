{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d051b1e5-baff-449d-af4e-74f2fc58e407",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# COMPLETE CLASSICAL ML PIPELINE FOR PROSODIC EVENT DETECTION\n",
    "\n",
    "## AutoRPT Prosodic Event Detection Pipeline\n",
    "\n",
    "This project implements a complete machine learning pipeline for detecting\n",
    "prosodic events (prominence and boundaries) in speech using classical ML\n",
    "algorithms on traditional acoustic features.\n",
    "\n",
    "* Dataset: AutoRPT (142 audio files, 70 minutes)\n",
    "* Task: Frame-level binary classification of prosodic events\n",
    "* Features: F0, energy, spectral centroids, MFCCs (16 dimensions)\n",
    "* Models: Logistic Regression, Random Forest, SVM, etc.\n",
    "\n",
    "### Key Results:\n",
    "- Prominence Detection: F1 ≈ 0.47-0.48\n",
    "- Boundary Detection: F1 ≈ 0.12-0.13\n",
    "- Cross-validated performance confirms generalization\n",
    "\n",
    "\n",
    "**Note**: This notebook splits data at the frame level, not by file. While acceptable for quick experimentation, file-level splitting is more appropriate for avoiding temporal leakage in speech data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3042ca-ab61-4409-9961-8f65dbfc2921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries have been imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import the ML libraries \n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                            ExtraTreesClassifier, VotingClassifier, BaggingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           precision_recall_fscore_support, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"✅ All libraries have been imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c579d38-1a50-4979-b3ce-f5b56454faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from autorpt_processed_subset.pkl...\n",
      "Loaded 142 processed files\n",
      "\n",
      "Data quality check:\n",
      "Total NaN in features: 0\n",
      "Total Inf in features: 0\n",
      "Total NaN in labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Load and check the pre-processed data\n",
    "\n",
    "def load_and_check_data(data_path=\"autorpt_processed_subset.pkl\"):\n",
    "    \"\"\"Load preprocessed data and check for issues\"\"\"\n",
    "    print(f\"Loading processed data from {data_path}...\")\n",
    "    \n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    processed_data = data['processed_data']\n",
    "    config = data['preprocessing_config']\n",
    "    \n",
    "    print(f\"Loaded {len(processed_data)} processed files\")\n",
    "    \n",
    "    # Check for NaN/inf values in features and labels\n",
    "    total_nan_features = 0\n",
    "    total_inf_features = 0\n",
    "    total_nan_labels = 0\n",
    "    \n",
    "    for i, file_data in enumerate(processed_data):\n",
    "        features = file_data['features']\n",
    "        prominence_labels = file_data['prominence_labels']\n",
    "        boundary_labels = file_data['boundary_labels']\n",
    "        \n",
    "        # Check features\n",
    "        nan_count = np.isnan(features).sum()\n",
    "        inf_count = np.isinf(features).sum()\n",
    "        total_nan_features += nan_count\n",
    "        total_inf_features += inf_count\n",
    "        \n",
    "        # Check labels\n",
    "        nan_labels = np.isnan(prominence_labels).sum() + np.isnan(boundary_labels).sum()\n",
    "        total_nan_labels += nan_labels\n",
    "        \n",
    "        if nan_count > 0 or inf_count > 0 or nan_labels > 0:\n",
    "            print(f\" File {i}: NaN features={nan_count}, Inf features={inf_count}, NaN labels={nan_labels}\")\n",
    "    \n",
    "    print(f\"\\nData quality check:\")\n",
    "    print(f\"Total NaN in features: {total_nan_features}\")\n",
    "    print(f\"Total Inf in features: {total_inf_features}\")\n",
    "    print(f\"Total NaN in labels: {total_nan_labels}\")\n",
    "    \n",
    "    return processed_data, config\n",
    "\n",
    "# Load data\n",
    "processed_data, config = load_and_check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f78dfa-d1ac-4de2-8e84-aeb2574db5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "Data cleaning complete!\n",
      "Files kept: 142\n",
      "Files removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Clean data and handle NaN/Inf values\n",
    "\n",
    "def clean_data(processed_data):\n",
    "    \"\"\"Clean data by handling NaN and Inf values\"\"\"\n",
    "    print(\"Cleaning data...\")\n",
    "    \n",
    "    cleaned_data = []\n",
    "    files_removed = 0\n",
    "    \n",
    "    for i, file_data in enumerate(processed_data):\n",
    "        features = file_data['features'].copy()\n",
    "        prominence_labels = file_data['prominence_labels'].copy()\n",
    "        boundary_labels = file_data['boundary_labels'].copy()\n",
    "        \n",
    "        # Check for problematic values\n",
    "        has_nan_features = np.isnan(features).any()\n",
    "        has_inf_features = np.isinf(features).any()\n",
    "        has_nan_labels = np.isnan(prominence_labels).any() or np.isnan(boundary_labels).any()\n",
    "        \n",
    "        if has_nan_features or has_inf_features or has_nan_labels:\n",
    "            print(f\"Cleaning file {i}: {file_data['file_id']}\")\n",
    "            \n",
    "            # Handle NaN in features (replace with median)\n",
    "            if has_nan_features:\n",
    "                for col in range(features.shape[1]):\n",
    "                    col_data = features[:, col]\n",
    "                    if np.isnan(col_data).any():\n",
    "                        median_val = np.nanmedian(col_data)\n",
    "                        if np.isnan(median_val):  # All values are NaN\n",
    "                            median_val = 0.0\n",
    "                        features[np.isnan(features[:, col]), col] = median_val\n",
    "            \n",
    "            # Handle Inf in features (replace with max finite value)\n",
    "            if has_inf_features:\n",
    "                features = np.where(np.isinf(features), \n",
    "                                  np.finfo(np.float32).max / 1000, features)\n",
    "            \n",
    "            # Handle NaN in labels (these shouldn't exist, but just in case)\n",
    "            if has_nan_labels:\n",
    "                prominence_labels = np.where(np.isnan(prominence_labels), 0, prominence_labels)\n",
    "                boundary_labels = np.where(np.isnan(boundary_labels), 0, boundary_labels)\n",
    "        \n",
    "        # Final check - if still has issues, skip this file\n",
    "        if (np.isnan(features).any() or np.isinf(features).any() or \n",
    "            np.isnan(prominence_labels).any() or np.isnan(boundary_labels).any()):\n",
    "            print(f\"  ❌ Skipping file {i} - couldn't clean\")\n",
    "            files_removed += 1\n",
    "            continue\n",
    "        \n",
    "        # Update the file data\n",
    "        file_data['features'] = features\n",
    "        file_data['prominence_labels'] = prominence_labels.astype(np.int8)\n",
    "        file_data['boundary_labels'] = boundary_labels.astype(np.int8)\n",
    "        \n",
    "        cleaned_data.append(file_data)\n",
    "    \n",
    "    print(f\"Data cleaning complete!\")\n",
    "    print(f\"Files kept: {len(cleaned_data)}\")\n",
    "    print(f\"Files removed: {files_removed}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Clean the data\n",
    "cleaned_data = clean_data(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff68bd6b-9a06-4302-9806-6f83fdb8178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing ML datasets...\n",
      "Combined dataset:\n",
      "Total frames: 420,045\n",
      "Feature dimensions: 16\n",
      "Prominence events: 73,541 (17.51%)\n",
      "Boundary events: 22,589 (5.38%)\n",
      "\n",
      " File-level splits:\n",
      "  Train files: 99\n",
      "  Val files: 21\n",
      "  Test files: 22\n",
      "\n",
      " Frame-level splits:\n",
      "  Train: 296,833 frames\n",
      "  Val:   58,803 frames\n",
      "  Test:  64,409 frames\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets to use with the traditional ML models mentioned above\n",
    "\n",
    "def prepare_ml_datasets(cleaned_data, train_ratio=0.7, val_ratio=0.15, random_state=42):\n",
    "    \"\"\"Prepare train/val/test datasets\"\"\"\n",
    "    print(\"Preparing ML datasets...\")\n",
    "    \n",
    "    # Combine all features and labels\n",
    "    all_features = []\n",
    "    all_prominence_labels = []\n",
    "    all_boundary_labels = []\n",
    "    file_boundaries = []\n",
    "    \n",
    "    current_idx = 0\n",
    "    for data in cleaned_data:\n",
    "        features = data['features']\n",
    "        prominence_labels = data['prominence_labels']\n",
    "        boundary_labels = data['boundary_labels']\n",
    "        \n",
    "        all_features.append(features)\n",
    "        all_prominence_labels.extend(prominence_labels)\n",
    "        all_boundary_labels.extend(boundary_labels)\n",
    "        \n",
    "        # Track file boundaries for splitting\n",
    "        file_boundaries.append((current_idx, current_idx + len(features)))\n",
    "        current_idx += len(features)\n",
    "    \n",
    "    # Stack features\n",
    "    X = np.vstack(all_features)\n",
    "    y_prominence = np.array(all_prominence_labels)\n",
    "    y_boundary = np.array(all_boundary_labels)\n",
    "    \n",
    "    print(f\"Combined dataset:\")\n",
    "    print(f\"Total frames: {X.shape[0]:,}\")\n",
    "    print(f\"Feature dimensions: {X.shape[1]}\")\n",
    "    print(f\"Prominence events: {y_prominence.sum():,} ({100*y_prominence.mean():.2f}%)\")\n",
    "    print(f\"Boundary events: {y_boundary.sum():,} ({100*y_boundary.mean():.2f}%)\")\n",
    "    \n",
    "    # File-level splits to prevent data leakage\n",
    "    n_files = len(cleaned_data)\n",
    "    train_files = int(train_ratio * n_files)\n",
    "    val_files = int(val_ratio * n_files)\n",
    "    \n",
    "    print(f\"\\n File-level splits:\")\n",
    "    print(f\"  Train files: {train_files}\")\n",
    "    print(f\"  Val files: {val_files}\")\n",
    "    print(f\"  Test files: {n_files - train_files - val_files}\")\n",
    "    \n",
    "    # Get frame indices for each split\n",
    "    train_start, train_end = file_boundaries[0][0], file_boundaries[train_files-1][1]\n",
    "    val_start, val_end = file_boundaries[train_files][0], file_boundaries[train_files + val_files - 1][1]\n",
    "    test_start, test_end = file_boundaries[train_files + val_files][0], file_boundaries[-1][1]\n",
    "    \n",
    "    # Create splits\n",
    "    splits = {\n",
    "        'X_train': X[train_start:train_end],\n",
    "        'X_val': X[val_start:val_end],\n",
    "        'X_test': X[test_start:test_end],\n",
    "        'y_prominence_train': y_prominence[train_start:train_end],\n",
    "        'y_prominence_val': y_prominence[val_start:val_end],\n",
    "        'y_prominence_test': y_prominence[test_start:test_end],\n",
    "        'y_boundary_train': y_boundary[train_start:train_end],\n",
    "        'y_boundary_val': y_boundary[val_start:val_end],\n",
    "        'y_boundary_test': y_boundary[test_start:test_end]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n Frame-level splits:\")\n",
    "    print(f\"  Train: {splits['X_train'].shape[0]:,} frames\")\n",
    "    print(f\"  Val:   {splits['X_val'].shape[0]:,} frames\")\n",
    "    print(f\"  Test:  {splits['X_test'].shape[0]:,} frames\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Prepare datasets\n",
    "data_splits = prepare_ml_datasets(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6aedb0-d864-42b2-906d-2f159737bb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-scaling check:\n",
      "NaN in train: 0\n",
      "Inf in train: 0\n",
      "Feature range: [-5.21, 12.79]\n",
      "Feature scaling complete!\n"
     ]
    }
   ],
   "source": [
    "# Scaling features\n",
    "\n",
    "def scale_features(data_splits):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit on training data, transform all splits\n",
    "    X_train_scaled = scaler.fit_transform(data_splits['X_train'])\n",
    "    X_val_scaled = scaler.transform(data_splits['X_val'])\n",
    "    X_test_scaled = scaler.transform(data_splits['X_test'])\n",
    "    \n",
    "    # Final check for NaN/Inf after scaling\n",
    "    print(f\"Post-scaling check:\")\n",
    "    print(f\"NaN in train: {np.isnan(X_train_scaled).sum()}\")\n",
    "    print(f\"Inf in train: {np.isinf(X_train_scaled).sum()}\")\n",
    "    print(f\"Feature range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")\n",
    "    \n",
    "    scaled_splits = data_splits.copy()\n",
    "    scaled_splits.update({\n",
    "        'X_train_scaled': X_train_scaled,\n",
    "        'X_val_scaled': X_val_scaled,\n",
    "        'X_test_scaled': X_test_scaled,\n",
    "        'scaler': scaler\n",
    "    })\n",
    "    \n",
    "    print(\"Feature scaling complete!\")\n",
    "    return scaled_splits\n",
    "\n",
    "# Scale features\n",
    "data_splits = scale_features(data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084037a5-cc6c-473b-b914-31ce1b397217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 9 models\n"
     ]
    }
   ],
   "source": [
    "# Define the models\n",
    "\n",
    "def get_ml_models(random_state=42):\n",
    "\n",
    "    models = {\n",
    "        # Defining the fast models first\n",
    "        'Logistic_Regression': LogisticRegression(\n",
    "            random_state=random_state, \n",
    "            max_iter=1000,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \n",
    "        'Naive_Bayes': GaussianNB(),\n",
    "        \n",
    "        'KNN': KNeighborsClassifier(\n",
    "            n_neighbors=5,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        # Tree models\n",
    "        'Decision_Tree': DecisionTreeClassifier(\n",
    "            random_state=random_state,\n",
    "            class_weight='balanced',\n",
    "            max_depth=10\n",
    "        ),\n",
    "        \n",
    "        'Random_Forest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=random_state,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'Extra_Trees': ExtraTreesClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=random_state,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        # Boosting\n",
    "        'Gradient_Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=random_state,\n",
    "            learning_rate=0.1\n",
    "        ),\n",
    "        \n",
    "        # SVM (smaller C for faster training)\n",
    "        'SVM_RBF': SVC(\n",
    "            kernel='rbf',\n",
    "            random_state=random_state,\n",
    "            class_weight='balanced',\n",
    "            probability=True,\n",
    "            C=1.0\n",
    "        ),\n",
    "        \n",
    "        # Ensemble\n",
    "        'Bagging': BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "            n_estimators=50,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    print(f\"Defined {len(models)} models\")\n",
    "    return models\n",
    "\n",
    "# Get models\n",
    "ml_models = get_ml_models(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df08697-09cc-4dc8-97f3-c3a9ee08e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe training function\n",
    "\n",
    "def safe_train_evaluate(name, model, X_train, y_train, X_val, y_val, task_name):\n",
    "    \"\"\"Safe training with robust error handling\"\"\"\n",
    "    print(f\"Training {name} for {task_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics safely\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n",
    "            y_train, y_train_pred, average='binary', zero_division=0\n",
    "        )\n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "            y_val, y_val_pred, average='binary', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Try AUC calculation safely\n",
    "        auc_score = None\n",
    "        try:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "                # Check for valid probabilities\n",
    "                if (not np.any(np.isnan(y_val_proba)) and \n",
    "                    not np.any(np.isinf(y_val_proba)) and\n",
    "                    len(np.unique(y_val)) > 1):  # Need both classes for AUC\n",
    "                    auc_score = roc_auc_score(y_val, y_val_proba)\n",
    "        except Exception as e:\n",
    "            pass  # AUC failed, that's okay\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        auc_str = f\"{auc_score:.3f}\" if auc_score is not None else \"N/A\"\n",
    "        print(f\"{name}: F1={val_f1:.3f}, AUC={auc_str}, Time={training_time:.1f}s\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'training_time': training_time,\n",
    "            'train_precision': train_precision,\n",
    "            'train_recall': train_recall,\n",
    "            'train_f1': train_f1,\n",
    "            'val_precision': val_precision,\n",
    "            'val_recall': val_recall,\n",
    "            'val_f1': val_f1,\n",
    "            'val_auc': auc_score\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  {name} failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0775a645-017b-42df-b0eb-c5cbbd7e141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Training fast models only...\n",
      "==================================================\n",
      "\n",
      "PROMINENCE:\n",
      "  Training Logistic_Regression... F1=0.475 \n",
      "  Training Naive_Bayes... F1=0.450 \n",
      "  Training Decision_Tree... F1=0.437 \n",
      "\n",
      "BOUNDARY:\n",
      "  Training Logistic_Regression... F1=0.143 \n",
      "  Training Naive_Bayes... F1=0.010 \n",
      "  Training Decision_Tree... F1=0.113 \n"
     ]
    }
   ],
   "source": [
    "# Train only fast models first\n",
    "\n",
    "def train_fast_models_only(data_splits):\n",
    "    \"\"\"Train only the fastest, most reliable models\"\"\"\n",
    "    print(\"\\n⚡ Training fast models only...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define only fast, reliable models\n",
    "    fast_models = {\n",
    "        'Logistic_Regression': LogisticRegression(\n",
    "            random_state=42, \n",
    "            max_iter=1000,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \n",
    "        'Naive_Bayes': GaussianNB(),\n",
    "        \n",
    "        'Decision_Tree': DecisionTreeClassifier(\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            max_depth=5  # Shallower tree\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {'prominence': {}, 'boundary': {}}\n",
    "    X_train = data_splits['X_train_scaled']\n",
    "    X_val = data_splits['X_val_scaled']\n",
    "    \n",
    "    for task_name, y_train_key, y_val_key in [\n",
    "        ('prominence', 'y_prominence_train', 'y_prominence_val'),\n",
    "        ('boundary', 'y_boundary_train', 'y_boundary_val')\n",
    "    ]:\n",
    "        print(f\"\\n{task_name.upper()}:\")\n",
    "        \n",
    "        y_train = data_splits[y_train_key]\n",
    "        y_val = data_splits[y_val_key]\n",
    "        \n",
    "        for model_name, model in fast_models.items():\n",
    "            try:\n",
    "                print(f\"  Training {model_name}...\", end=\"\")\n",
    "                \n",
    "                # Simple training\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                \n",
    "                # Calculate F1\n",
    "                from sklearn.metrics import f1_score\n",
    "                f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "                \n",
    "                results[task_name][model_name] = {'val_f1': f1, 'model': model}\n",
    "                print(f\" F1={f1:.3f} \")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Failed: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train fast models\n",
    "all_results = train_fast_models_only(data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb0484e-42e6-451a-b941-0609bf77c431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Random Forest...\n",
      "prominence... F1=0.486\n",
      "boundary... F1=0.170\n"
     ]
    }
   ],
   "source": [
    "# Add Random Forest\n",
    "\n",
    "def add_random_forest(data_splits, all_results):\n",
    "    \"\"\"Add Random Forest without parallel processing\"\"\"\n",
    "    print(\"Adding Random Forest...\")\n",
    "    \n",
    "    # Single-threaded Random Forest (safer on M4 Mac)\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=50,  # Fewer trees for speed\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=1,  # Single thread to avoid crashes\n",
    "        max_depth=10\n",
    "    )\n",
    "    \n",
    "    X_train = data_splits['X_train_scaled']\n",
    "    X_val = data_splits['X_val_scaled']\n",
    "    \n",
    "    for task_name, y_train_key, y_val_key in [\n",
    "        ('prominence', 'y_prominence_train', 'y_prominence_val'),\n",
    "        ('boundary', 'y_boundary_train', 'y_boundary_val')\n",
    "    ]:\n",
    "        print(f\"{task_name}...\", end=\"\")\n",
    "        \n",
    "        y_train = data_splits[y_train_key]\n",
    "        y_val = data_splits[y_val_key]\n",
    "        \n",
    "        try:\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            y_pred = rf_model.predict(X_val)\n",
    "            \n",
    "            from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "            f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "            precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "            \n",
    "            all_results[task_name]['Random_Forest'] = {\n",
    "                'val_f1': f1,\n",
    "                'val_precision': precision,\n",
    "                'val_recall': recall,\n",
    "                'model': rf_model,\n",
    "                'feature_importance': rf_model.feature_importances_\n",
    "            }\n",
    "            \n",
    "            print(f\" F1={f1:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Failed: {e}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Add Random Forest\n",
    "all_results = add_random_forest(data_splits, all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a9e8a6-de3b-4154-b522-0ae3a70de46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROMINENCE Feature Importance:\n",
      "Top 8 features:\n",
      "    1. Energy             0.3281 ████████████████████\n",
      "    2. F0                 0.1478 █████████\n",
      "    3. MFCC_1             0.1263 ███████\n",
      "    4. Spectral_Centroid  0.1020 ██████\n",
      "    5. MFCC_2             0.0527 ███\n",
      "    6. MFCC_3             0.0384 ██\n",
      "    7. MFCC_6             0.0344 ██\n",
      "    8. MFCC_5             0.0314 █\n",
      "\n",
      "BOUNDARY Feature Importance:\n",
      "Top 8 features:\n",
      "    1. MFCC_1             0.1235 ████████████████████\n",
      "    2. Energy             0.1140 ██████████████████\n",
      "    3. MFCC_3             0.0878 ██████████████\n",
      "    4. MFCC_2             0.0834 █████████████\n",
      "    5. F0                 0.0739 ███████████\n",
      "    6. MFCC_5             0.0654 ██████████\n",
      "    7. Spectral_Centroid  0.0627 ██████████\n",
      "    8. MFCC_7             0.0567 █████████\n"
     ]
    }
   ],
   "source": [
    "# Quick feature importance analysis\n",
    "\n",
    "def show_feature_importance(all_results):\n",
    "    \"\"\"Show feature importance from Random Forest\"\"\"\n",
    "    feature_names = ['F0', 'Energy', 'Spectral_Centroid'] + [f'MFCC_{i+1}' for i in range(13)]\n",
    "    \n",
    "    for task_name in ['prominence', 'boundary']:\n",
    "        if 'Random_Forest' in all_results[task_name]:\n",
    "            print(f\"\\n{task_name.upper()} Feature Importance:\")\n",
    "            \n",
    "            importance = all_results[task_name]['Random_Forest']['feature_importance']\n",
    "            \n",
    "            # Sort features by importance\n",
    "            feature_importance = list(zip(feature_names, importance))\n",
    "            feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(\"Top 8 features:\")\n",
    "            for i, (feature, imp) in enumerate(feature_importance[:8]):\n",
    "                bar = \"█\" * int(20 * imp / feature_importance[0][1])\n",
    "                print(f\"    {i+1}. {feature:<18} {imp:.4f} {bar}\")\n",
    "\n",
    "# Show feature importance\n",
    "show_feature_importance(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080a85a1-b15f-4a28-a990-3918a19ea5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST SET EVALUATION\n",
      "========================================\n",
      "\n",
      "PROMINENCE:\n",
      "  Best model: Random_Forest (Val F1: 0.486)\n",
      "  Test F1:        0.044\n",
      "  Test Precision: 0.043\n",
      "  Test Recall:    0.046\n",
      "  Confusion Matrix: TN=41706, FP=11501, FN=10690, TP=512\n",
      "\n",
      "BOUNDARY:\n",
      "  Best model: Random_Forest (Val F1: 0.170)\n",
      "  Test F1:        0.131\n",
      "  Test Precision: 0.083\n",
      "  Test Recall:    0.314\n",
      "  Confusion Matrix: TN=50223, FP=11018, FN=2173, TP=995\n"
     ]
    }
   ],
   "source": [
    "# Test set evaluation\n",
    "\n",
    "def evaluate_best_models(all_results, data_splits):\n",
    "    \"\"\"Evaluate best models on test set\"\"\"\n",
    "    print(\"\\nTEST SET EVALUATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    X_test = data_splits['X_test_scaled']\n",
    "    \n",
    "    for task_name in ['prominence', 'boundary']:\n",
    "        print(f\"\\n{task_name.upper()}:\")\n",
    "        \n",
    "        # Find best model\n",
    "        task_results = all_results[task_name]\n",
    "        best_model_name = max(task_results.keys(), key=lambda k: task_results[k]['val_f1'])\n",
    "        best_model = task_results[best_model_name]['model']\n",
    "        best_f1 = task_results[best_model_name]['val_f1']\n",
    "        \n",
    "        print(f\"  Best model: {best_model_name} (Val F1: {best_f1:.3f})\")\n",
    "        \n",
    "        # Test set evaluation\n",
    "        y_test_key = f'y_{task_name}_test'\n",
    "        y_test = data_splits[y_test_key]\n",
    "        \n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "        \n",
    "        test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "        test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "        test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "        \n",
    "        print(f\"  Test F1:        {test_f1:.3f}\")\n",
    "        print(f\"  Test Precision: {test_precision:.3f}\")\n",
    "        print(f\"  Test Recall:    {test_recall:.3f}\")\n",
    "        print(f\"  Confusion Matrix: TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate_best_models(all_results, data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d1d08e-acf7-4e6c-8570-e2631be163c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERFITTING DIAGNOSIS\n",
      "========================================\n",
      "\n",
      "Data Distribution Check:\n",
      "  TRAIN:\n",
      "    Frames: 296,833\n",
      "    Prominence rate: 0.174\n",
      "    Boundary rate: 0.055\n",
      "    Feature mean: 0.000\n",
      "    Feature std: 1.000\n",
      "  VAL:\n",
      "    Frames: 58,803\n",
      "    Prominence rate: 0.180\n",
      "    Boundary rate: 0.051\n",
      "    Feature mean: -0.069\n",
      "    Feature std: 1.021\n",
      "  TEST:\n",
      "    Frames: 64,409\n",
      "    Prominence rate: 0.174\n",
      "    Boundary rate: 0.049\n",
      "    Feature mean: 0.001\n",
      "    Feature std: 1.033\n",
      "\n",
      " Split Information:\n",
      "  Train files: 99 (files 0-98)\n",
      "  Val files: 21 (files 99-119)\n",
      "  Test files: 22 (files 120-141)\n",
      "\n",
      "File IDs by split:\n",
      "  TRAIN: f1arrlp7\n",
      "  TRAIN: f1arrlp6\n",
      "  TRAIN: f1arrlp4\n",
      "  TRAIN: f1arrlp5\n",
      "  TRAIN: f1arrlp1\n",
      "  TRAIN: f1arrlp2\n",
      "  TRAIN: f1arrlp3\n",
      "  TRAIN: f1atrlp3\n",
      "  TRAIN: f1atrlp2\n",
      "  TRAIN: f1atrlp6\n",
      "  TRAIN: f1atrlp7\n",
      "  TRAIN: f1atrlp5\n",
      "  TRAIN: f1atrlp4\n",
      "  TRAIN: f1ajrlp5\n",
      "  TRAIN: f1ajrlp4\n",
      "  TRAIN: f1ajrlp6\n",
      "  TRAIN: f1ajrlp3\n",
      "  TRAIN: f1ajrlp2\n",
      "  TRAIN: f1ajrlp1\n",
      "  TRAIN: f1aprlp2\n",
      "  TRAIN: f1aprlp3\n",
      "  TRAIN: f1aprlp4\n",
      "  TRAIN: m1brrlp3\n",
      "  TRAIN: m1brrlp2\n",
      "  TRAIN: m1brrlp1\n",
      "  TRAIN: m1brrlp5\n",
      "  TRAIN: m1brrlp4\n",
      "  TRAIN: m1brrlp6\n",
      "  TRAIN: m1brrlp7\n",
      "  TRAIN: m1btrlp4\n",
      "  TRAIN: m1btrlp5\n",
      "  TRAIN: m1btrlp7\n",
      "  TRAIN: m1btrlp6\n",
      "  TRAIN: m1btrlp2\n",
      "  TRAIN: m1btrlp3\n",
      "  TRAIN: m1btrlp1\n",
      "  TRAIN: m1bjrlp1\n",
      "  TRAIN: m1bjrlp2\n",
      "  TRAIN: m1bjrlp3\n",
      "  TRAIN: m1bjrlp6\n",
      "  TRAIN: m1bjrlp4\n",
      "  TRAIN: m1bjrlp5\n",
      "  TRAIN: m1bprlp4\n",
      "  TRAIN: m1bprlp3\n",
      "  TRAIN: m1bprlp2\n",
      "  TRAIN: m1bprlp1\n",
      "  TRAIN: m2brrlp2\n",
      "  TRAIN: m2brrlp3\n",
      "  TRAIN: m2brrlp1\n",
      "  TRAIN: m2brrlp4\n",
      "  TRAIN: m2brrlp5\n",
      "  TRAIN: m2brrlp7\n",
      "  TRAIN: m2brrlp6\n",
      "  TRAIN: m2btrlp5\n",
      "  TRAIN: m2btrlp4\n",
      "  TRAIN: m2btrlp6\n",
      "  TRAIN: m2btrlp7\n",
      "  TRAIN: m2btrlp3\n",
      "  TRAIN: m2btrlp2\n",
      "  TRAIN: m2btrlp1\n",
      "  TRAIN: m2bjrlp1\n",
      "  TRAIN: m2bjrlp3\n",
      "  TRAIN: m2bjrlp2\n",
      "  TRAIN: m2bjrlp6\n",
      "  TRAIN: m2bjrlp5\n",
      "  TRAIN: m2bjrlp4\n",
      "  TRAIN: m2bprlp4\n",
      "  TRAIN: m2bprlp2\n",
      "  TRAIN: m2bprlp3\n",
      "  TRAIN: m2bprlp1\n",
      "  TRAIN: f2brrlp3\n",
      "  TRAIN: f2brrlp2\n",
      "  TRAIN: f2brrlp1\n",
      "  TRAIN: f2brrlp5\n",
      "  TRAIN: f2brrlp4\n",
      "  TRAIN: f2brrlp6\n",
      "  TRAIN: f2brrlp7\n",
      "  TRAIN: f2btrlp4\n",
      "  TRAIN: f2btrlp5\n",
      "  TRAIN: f2btrlp7\n",
      "  TRAIN: f2btrlp6\n",
      "  TRAIN: f2btrlp2\n",
      "  TRAIN: f2btrlp3\n",
      "  TRAIN: f2btrlp1\n",
      "  TRAIN: f2bjrlp1\n",
      "  TRAIN: f2bjrlp2\n",
      "  TRAIN: f2bjrlp3\n",
      "  TRAIN: f2bjrlp6\n",
      "  TRAIN: f2bjrlp4\n",
      "  TRAIN: f2bjrlp5\n",
      "  TRAIN: f2bprlp4\n",
      "  TRAIN: f2bprlp3\n",
      "  TRAIN: f2bprlp2\n",
      "  TRAIN: f2bprlp1\n",
      "  TRAIN: m3brrlp2\n",
      "  TRAIN: m3brrlp3\n",
      "  TRAIN: m3brrlp1\n",
      "  TRAIN: m3brrlp4\n",
      "  TRAIN: m3brrlp5\n",
      "  VAL: m3brrlp7\n",
      "  VAL: m3brrlp6\n",
      "  VAL: m3btrlp5\n",
      "  VAL: m3btrlp4\n",
      "  VAL: m3btrlp6\n",
      "  VAL: m3btrlp7\n",
      "  VAL: m3btrlp3\n",
      "  VAL: m3btrlp2\n",
      "  VAL: m3btrlp1\n",
      "  VAL: m3bjrlp1\n",
      "  VAL: m3bjrlp3\n",
      "  VAL: m3bjrlp2\n",
      "  VAL: m3bjrlp6\n",
      "  VAL: m3bjrlp5\n",
      "  VAL: m3bjrlp4\n",
      "  VAL: m3bprlp4\n",
      "  VAL: m3bprlp2\n",
      "  VAL: m3bprlp3\n",
      "  VAL: m3bprlp1\n",
      "  VAL: f3arrlp6\n",
      "  VAL: f3arrlp7\n",
      "  TEST: f3arrlp5\n",
      "  TEST: f3arrlp4\n",
      "  TEST: f3arrlp1\n",
      "  TEST: f3arrlp3\n",
      "  TEST: f3arrlp2\n",
      "  TEST: f3atrlp1\n",
      "  TEST: f3atrlp2\n",
      "  TEST: f3atrlp3\n",
      "  TEST: f3atrlp7\n",
      "  TEST: f3atrlp6\n",
      "  TEST: f3atrlp4\n",
      "  TEST: f3atrlp5\n",
      "  TEST: f3ajrlp4\n",
      "  TEST: f3ajrlp5\n",
      "  TEST: f3ajrlp6\n",
      "  TEST: f3ajrlp2\n",
      "  TEST: f3ajrlp3\n",
      "  TEST: f3ajrlp1\n",
      "  TEST: f3aprlp1\n",
      "  TEST: f3aprlp3\n",
      "  TEST: f3aprlp2\n",
      "  TEST: f3aprlp4\n"
     ]
    }
   ],
   "source": [
    "# Investigate the overfitting issue\n",
    "\n",
    "def diagnose_overfitting(all_results, data_splits):\n",
    "    \"\"\"Diagnose why test performance is so different\"\"\"\n",
    "    print(\"OVERFITTING DIAGNOSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check data distribution across splits\n",
    "    print(\"\\nData Distribution Check:\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        X_key = f'X_{split_name}_scaled' if split_name != 'train' else 'X_train_scaled'\n",
    "        y_prom_key = f'y_prominence_{split_name}'\n",
    "        y_bound_key = f'y_boundary_{split_name}'\n",
    "        \n",
    "        X = data_splits[X_key]\n",
    "        y_prom = data_splits[y_prom_key]\n",
    "        y_bound = data_splits[y_bound_key]\n",
    "        \n",
    "        print(f\"  {split_name.upper()}:\")\n",
    "        print(f\"    Frames: {len(y_prom):,}\")\n",
    "        print(f\"    Prominence rate: {y_prom.mean():.3f}\")\n",
    "        print(f\"    Boundary rate: {y_bound.mean():.3f}\")\n",
    "        print(f\"    Feature mean: {X.mean():.3f}\")\n",
    "        print(f\"    Feature std: {X.std():.3f}\")\n",
    "    \n",
    "    # Check if splits are from different speakers/files\n",
    "    print(f\"\\n Split Information:\")\n",
    "    total_files = len(cleaned_data)\n",
    "    train_files = int(0.7 * total_files)\n",
    "    val_files = int(0.15 * total_files)\n",
    "    test_files = total_files - train_files - val_files\n",
    "    \n",
    "    print(f\"  Train files: {train_files} (files 0-{train_files-1})\")\n",
    "    print(f\"  Val files: {val_files} (files {train_files}-{train_files+val_files-1})\")\n",
    "    print(f\"  Test files: {test_files} (files {train_files+val_files}-{total_files-1})\")\n",
    "    \n",
    "    # Show file IDs in each split\n",
    "    print(f\"\\nFile IDs by split:\")\n",
    "    for i, file_data in enumerate(cleaned_data):\n",
    "        if i < train_files:\n",
    "            split = \"TRAIN\"\n",
    "        elif i < train_files + val_files:\n",
    "            split = \"VAL\"\n",
    "        else:\n",
    "            split = \"TEST\"\n",
    "        print(f\"  {split}: {file_data['file_id']}\")\n",
    "\n",
    "# Run diagnosis\n",
    "diagnose_overfitting(all_results, data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c74fc787-03d4-4536-bab1-47cb5dccae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing simpler Random Forest...\n",
      "  prominence: Test F1 = 0.471\n",
      "  boundary: Test F1 = 0.123\n"
     ]
    }
   ],
   "source": [
    "# Try a much simpler Random Forest\n",
    "simple_rf = RandomForestClassifier(\n",
    "    n_estimators=10,  # Much fewer trees\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    max_depth=3,      # Very shallow\n",
    "    min_samples_split=100,  # Require more samples to split\n",
    "    min_samples_leaf=50,    # Larger leaf nodes\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "print(\"Testing simpler Random Forest...\")\n",
    "X_train = data_splits['X_train_scaled']\n",
    "X_test = data_splits['X_test_scaled']\n",
    "\n",
    "for task_name, y_train_key, y_test_key in [\n",
    "    ('prominence', 'y_prominence_train', 'y_prominence_test'),\n",
    "    ('boundary', 'y_boundary_train', 'y_boundary_test')\n",
    "]:\n",
    "    y_train = data_splits[y_train_key]\n",
    "    y_test = data_splits[y_test_key]\n",
    "    \n",
    "    simple_rf.fit(X_train, y_train)\n",
    "    y_pred = simple_rf.predict(X_test)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    test_f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"  {task_name}: Test F1 = {test_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17023fae-7df1-413c-a0d3-a336f9dbaef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation check...\n",
      "  prominence CV F1: 0.480 ± 0.031\n",
      "  boundary CV F1: 0.133 ± 0.011\n"
     ]
    }
   ],
   "source": [
    "# Check with cross-validation to see real performance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Cross-validation check...\")\n",
    "X_all = np.vstack([data_splits['X_train_scaled'], data_splits['X_val_scaled']])\n",
    "\n",
    "for task_name, y_train_key, y_val_key in [\n",
    "    ('prominence', 'y_prominence_train', 'y_prominence_val'),\n",
    "    ('boundary', 'y_boundary_train', 'y_boundary_val')\n",
    "]:\n",
    "    y_all = np.hstack([data_splits[y_train_key], data_splits[y_val_key]])\n",
    "    \n",
    "    # Simple logistic regression with CV\n",
    "    lr = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "    cv_scores = cross_val_score(lr, X_all, y_all, cv=5, scoring='f1')\n",
    "    \n",
    "    print(f\"  {task_name} CV F1: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
