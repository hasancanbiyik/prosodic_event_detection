{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b10f3b-df76-47c3-84d2-6c50219f91bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Networks for Prosodic Event Detection\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# NEURAL NETWORKS FOR PROSODIC EVENT DETECTION\n",
    "# Complete pipeline: 1D CNN, LSTM, Bidirectional LSTM, and Transformer models\n",
    "# for detecting prosodic prominence and boundaries in speech\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Neural Networks for Prosodic Event Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. DATA LOADING AND PREPARATION\n",
    "\n",
    "class ProsodyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for prosodic event detection\n",
    "    Handles temporal sequences with proper padding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, prominence_labels, boundary_labels, sequence_length=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: List of feature arrays for each file\n",
    "            prominence_labels: List of prominence label arrays  \n",
    "            boundary_labels: List of boundary label arrays\n",
    "            sequence_length: Fixed sequence length for batching\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = []\n",
    "        self.prominence_targets = []\n",
    "        self.boundary_targets = []\n",
    "        \n",
    "        # Create fixed-length sequences from variable-length files\n",
    "        for file_features, prom_labels, bound_labels in zip(features, prominence_labels, boundary_labels):\n",
    "            # Split each file into sequences\n",
    "            file_sequences = self._create_sequences(file_features, prom_labels, bound_labels)\n",
    "            self.sequences.extend(file_sequences['features'])\n",
    "            self.prominence_targets.extend(file_sequences['prominence'])\n",
    "            self.boundary_targets.extend(file_sequences['boundary'])\n",
    "        \n",
    "        print(f\"ðŸ“Š Created {len(self.sequences)} sequences of length {sequence_length}\")\n",
    "    \n",
    "    def _create_sequences(self, features, prom_labels, bound_labels):\n",
    "        \"\"\"Create overlapping sequences from a single file\"\"\"\n",
    "        seq_features = []\n",
    "        seq_prominence = []\n",
    "        seq_boundary = []\n",
    "        \n",
    "        # Use overlapping windows with 50% overlap\n",
    "        hop_size = self.sequence_length // 2\n",
    "        \n",
    "        for i in range(0, len(features) - self.sequence_length + 1, hop_size):\n",
    "            end_idx = i + self.sequence_length\n",
    "            \n",
    "            seq_features.append(features[i:end_idx])\n",
    "            seq_prominence.append(prom_labels[i:end_idx])\n",
    "            seq_boundary.append(bound_labels[i:end_idx])\n",
    "        \n",
    "        return {\n",
    "            'features': seq_features,\n",
    "            'prominence': seq_prominence, \n",
    "            'boundary': seq_boundary\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.FloatTensor(self.sequences[idx]),\n",
    "            'prominence': torch.LongTensor(self.prominence_targets[idx]),\n",
    "            'boundary': torch.LongTensor(self.boundary_targets[idx])\n",
    "        }\n",
    "\n",
    "def load_and_prepare_data(data_path=\"autorpt_processed_subset.pkl\"):\n",
    "    \"\"\"Load data and prepare for neural networks\"\"\"\n",
    "    print(\"ðŸ“‚ Loading processed data...\")\n",
    "    \n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    processed_data = data['processed_data']\n",
    "    print(f\"âœ… Loaded {len(processed_data)} files\")\n",
    "    \n",
    "    # Separate by splits (same as classical ML)\n",
    "    n_files = len(processed_data)\n",
    "    train_files = int(0.7 * n_files)  # 99 files\n",
    "    val_files = int(0.15 * n_files)   # 21 files\n",
    "    \n",
    "    # Extract features and labels by split\n",
    "    splits = {\n",
    "        'train': processed_data[:train_files],\n",
    "        'val': processed_data[train_files:train_files + val_files],\n",
    "        'test': processed_data[train_files + val_files:]\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ“Š Split sizes: Train={len(splits['train'])}, Val={len(splits['val'])}, Test={len(splits['test'])}\")\n",
    "    \n",
    "    # Scale features globally\n",
    "    print(\"ðŸ”§ Scaling features...\")\n",
    "    all_features = np.vstack([d['features'] for d in processed_data])\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_features)\n",
    "    \n",
    "    # Apply scaling to each split\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        for file_data in splits[split_name]:\n",
    "            file_data['features'] = scaler.transform(file_data['features'])\n",
    "    \n",
    "    return splits, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94921a42-3c43-433a-a743-692488f558fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DIAGNOSING NEURAL NETWORK ISSUES\n",
      "==================================================\n",
      "ðŸ” RUNNING DEBUG EXPERIMENT\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š CLASS DISTRIBUTION ANALYSIS\n",
      "----------------------------------------\n",
      "TRAIN: 296,833 frames\n",
      "  Prominence: 51,730 / 296,833 = 17.4%\n",
      "  Boundary:   16,393 / 296,833 = 5.5%\n",
      "\n",
      "VAL  : 58,803 frames\n",
      "  Prominence: 10,609 / 58,803 = 18.0%\n",
      "  Boundary:   3,028 / 58,803 = 5.1%\n",
      "\n",
      "TEST : 64,409 frames\n",
      "  Prominence: 11,202 / 64,409 = 17.4%\n",
      "  Boundary:   3,168 / 64,409 = 4.9%\n",
      "\n",
      "\n",
      "ðŸ” SEQUENCE LABEL ANALYSIS\n",
      "----------------------------------------\n",
      "Sample sequences (first 10):\n",
      "  Seq 0: Prom  0/50 â†’ 0 | Bound  0/50 â†’ 0\n",
      "  Seq 1: Prom  0/50 â†’ 0 | Bound  0/50 â†’ 0\n",
      "  Seq 2: Prom  0/50 â†’ 0 | Bound  0/50 â†’ 0\n",
      "  Seq 3: Prom 10/50 â†’ 0 | Bound  0/50 â†’ 0\n",
      "  Seq 4: Prom 13/50 â†’ 0 | Bound  0/50 â†’ 0\n",
      "  Seq 5: Prom 12/50 â†’ 0 | Bound  0/50 â†’ 0\n",
      "  Seq 6: Prom  9/50 â†’ 0 | Bound  8/50 â†’ 0\n",
      "  Seq 7: Prom  0/50 â†’ 0 | Bound 11/50 â†’ 0\n",
      "  Seq 8: Prom 11/50 â†’ 0 | Bound  3/50 â†’ 0\n",
      "  Seq 9: Prom 11/50 â†’ 0 | Bound  0/50 â†’ 0\n",
      "\n",
      "ðŸ“Š SEQUENCE-LEVEL DISTRIBUTION:\n",
      "  Total sequences: 11,736\n",
      "  Positive prominence sequences: 14 (0.1%)\n",
      "  Positive boundary sequences: 0 (0.0%)\n",
      "\n",
      "ðŸ§ª TESTING SIMPLE BASELINE MODEL\n",
      "----------------------------------------\n",
      "ðŸš€ Training with improved setup for 15 epochs...\n",
      "Epoch  1/15 | Loss: 4.5005 | Prom F1: 0.146/0.000 | Bound F1: 0.210/0.171\n",
      "Epoch  2/15 | Loss: 0.0896 | Prom F1: 0.110/0.007 | Bound F1: 0.219/0.143\n",
      "Epoch  3/15 | Loss: 0.0390 | Prom F1: 0.067/0.000 | Bound F1: 0.185/0.364\n",
      "Epoch  4/15 | Loss: 0.0364 | Prom F1: 0.065/0.000 | Bound F1: 0.167/0.254\n",
      "Epoch  5/15 | Loss: 0.0367 | Prom F1: 0.050/0.000 | Bound F1: 0.173/0.000\n",
      "  Prom CM: TN=10160, FP=356, FN=1180, TP=40\n",
      "  Bound CM: TN=7924, FP=1486, FN=1966, TP=360\n",
      "Epoch  6/15 | Loss: 0.0365 | Prom F1: 0.069/0.000 | Bound F1: 0.151/0.000\n",
      "Epoch  7/15 | Loss: 0.0354 | Prom F1: 0.045/0.000 | Bound F1: 0.166/0.000\n",
      "Epoch  8/15 | Loss: 0.0340 | Prom F1: 0.045/0.000 | Bound F1: 0.150/0.000\n",
      "Epoch  9/15 | Loss: 0.0328 | Prom F1: 0.051/0.000 | Bound F1: 0.135/0.000\n",
      "Epoch 10/15 | Loss: 0.0319 | Prom F1: 0.040/0.000 | Bound F1: 0.095/0.005\n",
      "  Prom CM: TN=10327, FP=189, FN=1191, TP=29\n",
      "  Bound CM: TN=8943, FP=467, FN=2187, TP=139\n",
      "Epoch 11/15 | Loss: 0.0291 | Prom F1: 0.002/0.000 | Bound F1: 0.052/0.000\n",
      "Epoch 12/15 | Loss: 0.0282 | Prom F1: 0.002/0.007 | Bound F1: 0.005/0.000\n",
      "Epoch 13/15 | Loss: 0.0275 | Prom F1: 0.002/0.000 | Bound F1: 0.000/0.000\n",
      "Epoch 14/15 | Loss: 0.0275 | Prom F1: 0.005/0.014 | Bound F1: 0.015/0.000\n",
      "Epoch 15/15 | Loss: 0.0274 | Prom F1: 0.008/0.000 | Bound F1: 0.001/0.000\n",
      "  Prom CM: TN=10504, FP=12, FN=1215, TP=5\n",
      "  Bound CM: TN=9410, FP=0, FN=2325, TP=1\n",
      "\n",
      "âœ… Debug experiment complete!\n",
      "ðŸ’¡ Key insight: Lower thresholds + Focal loss should help with extreme imbalance\n"
     ]
    }
   ],
   "source": [
    "# NEURAL NETWORKS DEBUG & FIX\n",
    "# =====================================================\n",
    "# Let's diagnose and fix the F1=0.000 issue\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"ðŸ” DIAGNOSING NEURAL NETWORK ISSUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =====================================================\n",
    "# 1. DIAGNOSTIC FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def diagnose_class_distribution(splits):\n",
    "    \"\"\"Analyze the extreme class imbalance issue\"\"\"\n",
    "    print(\"\\nðŸ“Š CLASS DISTRIBUTION ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        all_prom = np.hstack([d['prominence_labels'] for d in splits[split_name]])\n",
    "        all_bound = np.hstack([d['boundary_labels'] for d in splits[split_name]])\n",
    "        \n",
    "        prom_rate = all_prom.mean()\n",
    "        bound_rate = all_bound.mean()\n",
    "        \n",
    "        print(f\"{split_name.upper():5s}: {len(all_prom):,} frames\")\n",
    "        print(f\"  Prominence: {all_prom.sum():,} / {len(all_prom):,} = {prom_rate:.1%}\")\n",
    "        print(f\"  Boundary:   {all_bound.sum():,} / {len(all_bound):,} = {bound_rate:.1%}\")\n",
    "        print()\n",
    "\n",
    "def diagnose_sequence_labels(seq_datasets):\n",
    "    \"\"\"Check what happens when we convert to sequence labels\"\"\"\n",
    "    print(\"\\nðŸ” SEQUENCE LABEL ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    train_dataset = seq_datasets['train']\n",
    "    \n",
    "    # Sample a few sequences to see the label conversion\n",
    "    sample_sequences = []\n",
    "    for i in range(min(10, len(train_dataset))):\n",
    "        sample = train_dataset[i]\n",
    "        prom_seq = sample['prominence'].numpy()\n",
    "        bound_seq = sample['boundary'].numpy()\n",
    "        \n",
    "        # This is what our training does:\n",
    "        prom_label = int(prom_seq.mean() > 0.5)\n",
    "        bound_label = int(bound_seq.mean() > 0.5)\n",
    "        \n",
    "        sample_sequences.append({\n",
    "            'prom_frames': prom_seq.sum(),\n",
    "            'bound_frames': bound_seq.sum(),\n",
    "            'prom_label': prom_label,\n",
    "            'bound_label': bound_label\n",
    "        })\n",
    "    \n",
    "    print(\"Sample sequences (first 10):\")\n",
    "    for i, seq in enumerate(sample_sequences):\n",
    "        print(f\"  Seq {i}: Prom {seq['prom_frames']:2d}/50 â†’ {seq['prom_label']} | \"\n",
    "              f\"Bound {seq['bound_frames']:2d}/50 â†’ {seq['bound_label']}\")\n",
    "    \n",
    "    # Check overall sequence-level distribution\n",
    "    all_prom_labels = []\n",
    "    all_bound_labels = []\n",
    "    \n",
    "    for i in range(len(train_dataset)):\n",
    "        sample = train_dataset[i]\n",
    "        prom_seq = sample['prominence'].numpy()\n",
    "        bound_seq = sample['boundary'].numpy()\n",
    "        \n",
    "        prom_label = int(prom_seq.mean() > 0.5)\n",
    "        bound_label = int(bound_seq.mean() > 0.5)\n",
    "        \n",
    "        all_prom_labels.append(prom_label)\n",
    "        all_bound_labels.append(bound_label)\n",
    "    \n",
    "    prom_pos = np.sum(all_prom_labels)\n",
    "    bound_pos = np.sum(all_bound_labels)\n",
    "    total = len(all_prom_labels)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š SEQUENCE-LEVEL DISTRIBUTION:\")\n",
    "    print(f\"  Total sequences: {total:,}\")\n",
    "    print(f\"  Positive prominence sequences: {prom_pos:,} ({100*prom_pos/total:.1f}%)\")\n",
    "    print(f\"  Positive boundary sequences: {bound_pos:,} ({100*bound_pos/total:.1f}%)\")\n",
    "    \n",
    "    return all_prom_labels, all_bound_labels\n",
    "\n",
    "# =====================================================\n",
    "# 2. SIMPLE BASELINE MODEL\n",
    "# =====================================================\n",
    "\n",
    "class SimpleBaseline(nn.Module):\n",
    "    \"\"\"Ultra-simple baseline to test if learning works at all\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features=16):\n",
    "        super().__init__()\n",
    "        # Just global average pooling + linear layers\n",
    "        self.prominence_head = nn.Sequential(\n",
    "            nn.Linear(input_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "        self.boundary_head = nn.Sequential(\n",
    "            nn.Linear(input_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Global average over sequence dimension\n",
    "        x_avg = torch.mean(x, dim=1)  # (batch, features)\n",
    "        \n",
    "        prom_out = self.prominence_head(x_avg)\n",
    "        bound_out = self.boundary_head(x_avg)\n",
    "        \n",
    "        return prom_out, bound_out\n",
    "\n",
    "# =====================================================\n",
    "# 3. FOCAL LOSS FOR EXTREME IMBALANCE\n",
    "# =====================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal loss to handle extreme class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# =====================================================\n",
    "# 4. IMPROVED TRAINER\n",
    "# =====================================================\n",
    "\n",
    "class ImprovedTrainer:\n",
    "    \"\"\"Trainer specifically designed for extreme class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cpu', use_focal_loss=True):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Use focal loss or weighted CrossEntropy\n",
    "        if use_focal_loss:\n",
    "            self.prominence_criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "            self.boundary_criterion = FocalLoss(alpha=0.1, gamma=3.0)  # More aggressive for boundaries\n",
    "        else:\n",
    "            # Moderate weights\n",
    "            prom_weight = torch.FloatTensor([1.0, 5.0]).to(device)\n",
    "            bound_weight = torch.FloatTensor([1.0, 10.0]).to(device)\n",
    "            \n",
    "            self.prominence_criterion = nn.CrossEntropyLoss(weight=prom_weight)\n",
    "            self.boundary_criterion = nn.CrossEntropyLoss(weight=bound_weight)\n",
    "        \n",
    "        # Lower learning rate with different optimizer\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-3)\n",
    "        \n",
    "        self.history = []\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_prom_preds, all_prom_targets = [], []\n",
    "        all_bound_preds, all_bound_targets = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(self.device)\n",
    "            prom_targets = batch['prominence'].to(self.device)\n",
    "            bound_targets = batch['boundary'].to(self.device)\n",
    "            \n",
    "            # Convert to sequence labels with LOWER threshold\n",
    "            prom_seq_labels = (prom_targets.float().mean(dim=1) > 0.3).long()  # 30% threshold\n",
    "            bound_seq_labels = (bound_targets.float().mean(dim=1) > 0.2).long()  # 20% threshold\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            prom_outputs, bound_outputs = self.model(features)\n",
    "            \n",
    "            # Calculate losses\n",
    "            prom_loss = self.prominence_criterion(prom_outputs, prom_seq_labels)\n",
    "            bound_loss = self.boundary_criterion(bound_outputs, bound_seq_labels)\n",
    "            total_loss_batch = prom_loss + bound_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            # Collect predictions\n",
    "            prom_preds = torch.argmax(prom_outputs, dim=1).cpu().numpy()\n",
    "            bound_preds = torch.argmax(bound_outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_prom_preds.extend(prom_preds)\n",
    "            all_prom_targets.extend(prom_seq_labels.cpu().numpy())\n",
    "            all_bound_preds.extend(bound_preds)\n",
    "            all_bound_targets.extend(bound_seq_labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_prom_f1 = f1_score(all_prom_targets, all_prom_preds, zero_division=0)\n",
    "        train_bound_f1 = f1_score(all_bound_targets, all_bound_preds, zero_division=0)\n",
    "        \n",
    "        # Print diagnostics\n",
    "        prom_cm = confusion_matrix(all_prom_targets, all_prom_preds)\n",
    "        bound_cm = confusion_matrix(all_bound_targets, all_bound_preds)\n",
    "        \n",
    "        return total_loss / len(train_loader), train_prom_f1, train_bound_f1, prom_cm, bound_cm\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        all_prom_preds, all_prom_targets = [], []\n",
    "        all_bound_preds, all_bound_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(self.device)\n",
    "                prom_targets = batch['prominence'].to(self.device)\n",
    "                bound_targets = batch['boundary'].to(self.device)\n",
    "                \n",
    "                # Same thresholds as training\n",
    "                prom_seq_labels = (prom_targets.float().mean(dim=1) > 0.3).long()\n",
    "                bound_seq_labels = (bound_targets.float().mean(dim=1) > 0.2).long()\n",
    "                \n",
    "                # Forward pass\n",
    "                prom_outputs, bound_outputs = self.model(features)\n",
    "                \n",
    "                # Collect predictions\n",
    "                prom_preds = torch.argmax(prom_outputs, dim=1).cpu().numpy()\n",
    "                bound_preds = torch.argmax(bound_outputs, dim=1).cpu().numpy()\n",
    "                \n",
    "                all_prom_preds.extend(prom_preds)\n",
    "                all_prom_targets.extend(prom_seq_labels.cpu().numpy())\n",
    "                all_bound_preds.extend(bound_preds)\n",
    "                all_bound_targets.extend(bound_seq_labels.cpu().numpy())\n",
    "        \n",
    "        val_prom_f1 = f1_score(all_prom_targets, all_prom_preds, zero_division=0)\n",
    "        val_bound_f1 = f1_score(all_bound_targets, all_bound_preds, zero_division=0)\n",
    "        \n",
    "        return val_prom_f1, val_bound_f1\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=15):\n",
    "        print(f\"ðŸš€ Training with improved setup for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss, train_prom_f1, train_bound_f1, prom_cm, bound_cm = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validate\n",
    "            val_prom_f1, val_bound_f1 = self.evaluate(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "                  f\"Loss: {train_loss:.4f} | \"\n",
    "                  f\"Prom F1: {train_prom_f1:.3f}/{val_prom_f1:.3f} | \"\n",
    "                  f\"Bound F1: {train_bound_f1:.3f}/{val_bound_f1:.3f}\")\n",
    "            \n",
    "            # Print confusion matrices every 5 epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"  Prom CM: TN={prom_cm[0,0]}, FP={prom_cm[0,1]}, FN={prom_cm[1,0]}, TP={prom_cm[1,1]}\")\n",
    "                print(f\"  Bound CM: TN={bound_cm[0,0]}, FP={bound_cm[0,1]}, FN={bound_cm[1,0]}, TP={bound_cm[1,1]}\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. MAIN DEBUGGING EXPERIMENT\n",
    "# =====================================================\n",
    "\n",
    "def run_debug_experiment():\n",
    "    print(\"ðŸ” RUNNING DEBUG EXPERIMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load data\n",
    "    with open(\"autorpt_processed_subset.pkl\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    processed_data = data['processed_data']\n",
    "    \n",
    "    # Same splits as before\n",
    "    n_files = len(processed_data)\n",
    "    train_files = int(0.7 * n_files)\n",
    "    val_files = int(0.15 * n_files)\n",
    "    \n",
    "    splits = {\n",
    "        'train': processed_data[:train_files],\n",
    "        'val': processed_data[train_files:train_files + val_files],\n",
    "        'test': processed_data[train_files + val_files:]\n",
    "    }\n",
    "    \n",
    "    # Diagnose class distribution\n",
    "    diagnose_class_distribution(splits)\n",
    "    \n",
    "    # Create datasets\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    class SimpleProsodyDataset:\n",
    "        def __init__(self, splits_data, sequence_length=50):\n",
    "            self.sequences = []\n",
    "            self.prominence_targets = []\n",
    "            self.boundary_targets = []\n",
    "            \n",
    "            for file_data in splits_data:\n",
    "                features = file_data['features']\n",
    "                prom_labels = file_data['prominence_labels']\n",
    "                bound_labels = file_data['boundary_labels']\n",
    "                \n",
    "                # Create overlapping sequences\n",
    "                hop_size = sequence_length // 2\n",
    "                for i in range(0, len(features) - sequence_length + 1, hop_size):\n",
    "                    end_idx = i + sequence_length\n",
    "                    \n",
    "                    self.sequences.append(features[i:end_idx])\n",
    "                    self.prominence_targets.append(prom_labels[i:end_idx])\n",
    "                    self.boundary_targets.append(bound_labels[i:end_idx])\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.sequences)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                'features': torch.FloatTensor(self.sequences[idx]),\n",
    "                'prominence': torch.LongTensor(self.prominence_targets[idx]),\n",
    "                'boundary': torch.LongTensor(self.boundary_targets[idx])\n",
    "            }\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SimpleProsodyDataset(splits['train'])\n",
    "    val_dataset = SimpleProsodyDataset(splits['val'])\n",
    "    \n",
    "    # Diagnose sequence labels\n",
    "    seq_datasets = {'train': train_dataset, 'val': val_dataset}\n",
    "    all_prom_labels, all_bound_labels = diagnose_sequence_labels(seq_datasets)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Test simple baseline\n",
    "    print(f\"\\nðŸ§ª TESTING SIMPLE BASELINE MODEL\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    model = SimpleBaseline(input_features=16)\n",
    "    trainer = ImprovedTrainer(model, device='cpu', use_focal_loss=True)\n",
    "    \n",
    "    # Train\n",
    "    trainer.train(train_loader, val_loader, epochs=15)\n",
    "    \n",
    "    print(f\"\\nâœ… Debug experiment complete!\")\n",
    "    print(f\"ðŸ’¡ Key insight: Lower thresholds + Focal loss should help with extreme imbalance\")\n",
    "\n",
    "# =====================================================\n",
    "# RUN DEBUG\n",
    "# =====================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_debug_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dceac9cc-0ca3-4d4e-b119-fb30fc0857d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ FINAL NEURAL PROSODY DETECTION\n",
      "============================================================\n",
      "ðŸš€ PRODUCTION NEURAL NETWORK EXPERIMENT\n",
      "==================================================\n",
      "ðŸ“‚ Loading data...\n",
      "ðŸ“Š Files: Train=99, Val=21, Test=22\n",
      "\n",
      "ðŸ“¦ Creating optimized datasets...\n",
      "  ðŸ“ Creating sequences with length 50\n",
      "  ðŸŽ¯ Prominence threshold: 15.0%\n",
      "  ðŸŽ¯ Boundary threshold: 10.0%\n",
      "  ðŸ“Š Created 18,284 sequences\n",
      "  ðŸ“Š Positive prominence: 12,308 (67.3%)\n",
      "  ðŸ“Š Positive boundary: 4,726 (25.8%)\n",
      "  ðŸ“ Creating sequences with length 50\n",
      "  ðŸŽ¯ Prominence threshold: 15.0%\n",
      "  ðŸŽ¯ Boundary threshold: 10.0%\n",
      "  ðŸ“Š Created 3,622 sequences\n",
      "  ðŸ“Š Positive prominence: 2,480 (68.5%)\n",
      "  ðŸ“Š Positive boundary: 855 (23.6%)\n",
      "  ðŸ“ Creating sequences with length 50\n",
      "  ðŸŽ¯ Prominence threshold: 15.0%\n",
      "  ðŸŽ¯ Boundary threshold: 10.0%\n",
      "  ðŸ“Š Created 3,970 sequences\n",
      "  ðŸ“Š Positive prominence: 2,653 (66.8%)\n",
      "  ðŸ“Š Positive boundary: 919 (23.1%)\n",
      "ðŸ”§ Using device: cpu\n",
      "\n",
      "ðŸš€ Training Production_CNN...\n",
      "----------------------------------------\n",
      "ðŸš€ Training for 25 epochs...\n",
      "Epoch  1/25 | Time: 3.6s | Loss: 1.0422/1.0572 | Prom F1: 0.838/0.796 | Bound F1: 0.242/0.478\n",
      "Epoch  2/25 | Time: 3.7s | Loss: 0.9547/0.9497 | Prom F1: 0.848/0.840 | Bound F1: 0.490/0.429\n",
      "Epoch  3/25 | Time: 3.8s | Loss: 0.9247/0.9442 | Prom F1: 0.851/0.844 | Bound F1: 0.543/0.540\n",
      "Epoch  4/25 | Time: 3.8s | Loss: 0.9146/0.9214 | Prom F1: 0.849/0.844 | Bound F1: 0.557/0.509\n",
      "Epoch  5/25 | Time: 3.9s | Loss: 0.8945/0.9514 | Prom F1: 0.852/0.820 | Bound F1: 0.566/0.615\n",
      "Epoch  6/25 | Time: 3.7s | Loss: 0.8859/0.9297 | Prom F1: 0.854/0.839 | Bound F1: 0.576/0.575\n",
      "Epoch  7/25 | Time: 3.6s | Loss: 0.8808/0.9341 | Prom F1: 0.856/0.822 | Bound F1: 0.582/0.602\n",
      "Epoch  8/25 | Time: 3.6s | Loss: 0.8682/1.0187 | Prom F1: 0.856/0.766 | Bound F1: 0.592/0.622\n",
      "Epoch  9/25 | Time: 3.8s | Loss: 0.8632/0.9406 | Prom F1: 0.854/0.797 | Bound F1: 0.593/0.567\n",
      "Epoch 10/25 | Time: 3.9s | Loss: 0.8606/0.9166 | Prom F1: 0.857/0.819 | Bound F1: 0.580/0.568\n",
      "Epoch 11/25 | Time: 4.0s | Loss: 0.8510/0.9143 | Prom F1: 0.857/0.838 | Bound F1: 0.599/0.569\n",
      "Epoch 12/25 | Time: 3.7s | Loss: 0.8484/0.9038 | Prom F1: 0.860/0.827 | Bound F1: 0.592/0.597\n",
      "Epoch 13/25 | Time: 3.6s | Loss: 0.8445/0.9108 | Prom F1: 0.858/0.831 | Bound F1: 0.597/0.621\n",
      "Epoch 14/25 | Time: 3.6s | Loss: 0.8400/0.9160 | Prom F1: 0.861/0.810 | Bound F1: 0.601/0.593\n",
      "Epoch 15/25 | Time: 3.8s | Loss: 0.8393/0.8976 | Prom F1: 0.859/0.834 | Bound F1: 0.605/0.600\n",
      "Epoch 16/25 | Time: 3.9s | Loss: 0.8290/0.9193 | Prom F1: 0.862/0.808 | Bound F1: 0.609/0.620\n",
      "Epoch 17/25 | Time: 3.8s | Loss: 0.8278/0.9271 | Prom F1: 0.861/0.798 | Bound F1: 0.606/0.606\n",
      "Epoch 18/25 | Time: 3.8s | Loss: 0.8256/0.8873 | Prom F1: 0.862/0.835 | Bound F1: 0.619/0.592\n",
      "Epoch 19/25 | Time: 3.8s | Loss: 0.8193/0.8800 | Prom F1: 0.863/0.837 | Bound F1: 0.614/0.566\n",
      "Epoch 20/25 | Time: 3.6s | Loss: 0.8204/0.9026 | Prom F1: 0.862/0.829 | Bound F1: 0.620/0.604\n",
      "Epoch 21/25 | Time: 3.6s | Loss: 0.8118/0.9063 | Prom F1: 0.864/0.828 | Bound F1: 0.621/0.582\n",
      "ðŸ’¤ Early stopping at epoch 21\n",
      "âœ… Training complete! Best validation F1: 0.726\n",
      "ðŸŽ¯ Production_CNN Final Test Results:\n",
      "  Prominence F1: 0.852\n",
      "  Boundary F1: 0.551\n",
      "\n",
      "ðŸš€ Training Production_RNN...\n",
      "----------------------------------------\n",
      "ðŸš€ Training for 25 epochs...\n",
      "Epoch  1/25 | Time: 4.4s | Loss: 1.1203/1.0424 | Prom F1: 0.823/0.832 | Bound F1: 0.043/0.044\n",
      "Epoch  2/25 | Time: 4.4s | Loss: 1.0375/1.0153 | Prom F1: 0.836/0.837 | Bound F1: 0.348/0.322\n",
      "Epoch  3/25 | Time: 4.3s | Loss: 1.0095/1.0154 | Prom F1: 0.838/0.827 | Bound F1: 0.418/0.293\n",
      "Epoch  4/25 | Time: 4.3s | Loss: 0.9908/1.0149 | Prom F1: 0.839/0.835 | Bound F1: 0.462/0.528\n",
      "Epoch  5/25 | Time: 4.2s | Loss: 0.9771/0.9889 | Prom F1: 0.842/0.839 | Bound F1: 0.491/0.507\n",
      "Epoch  6/25 | Time: 4.3s | Loss: 0.9705/0.9943 | Prom F1: 0.843/0.832 | Bound F1: 0.488/0.468\n",
      "Epoch  7/25 | Time: 4.3s | Loss: 0.9610/0.9949 | Prom F1: 0.844/0.843 | Bound F1: 0.499/0.481\n",
      "Epoch  8/25 | Time: 4.3s | Loss: 0.9562/1.0040 | Prom F1: 0.845/0.841 | Bound F1: 0.516/0.457\n",
      "Epoch  9/25 | Time: 4.3s | Loss: 0.9543/1.0340 | Prom F1: 0.846/0.799 | Bound F1: 0.518/0.531\n",
      "Epoch 10/25 | Time: 4.3s | Loss: 0.9517/0.9995 | Prom F1: 0.846/0.827 | Bound F1: 0.519/0.415\n",
      "Epoch 11/25 | Time: 4.3s | Loss: 0.9482/0.9911 | Prom F1: 0.847/0.833 | Bound F1: 0.523/0.492\n",
      "Epoch 12/25 | Time: 4.3s | Loss: 0.9361/0.9981 | Prom F1: 0.846/0.828 | Bound F1: 0.527/0.477\n",
      "ðŸ’¤ Early stopping at epoch 12\n",
      "âœ… Training complete! Best validation F1: 0.681\n",
      "ðŸŽ¯ Production_RNN Final Test Results:\n",
      "  Prominence F1: 0.837\n",
      "  Boundary F1: 0.452\n",
      "\n",
      "ðŸ“Š FINAL NEURAL NETWORK RESULTS\n",
      "==================================================\n",
      "Model           | Prom F1  | Bound F1 | Avg F1  \n",
      "--------------------------------------------------\n",
      "Production_CNN  | 0.852    | 0.551    | 0.702   \n",
      "Production_RNN  | 0.837    | 0.452    | 0.644   \n",
      "\n",
      "ðŸ“Š NEURAL vs CLASSICAL COMPARISON\n",
      "========================================\n",
      "Classical ML Baselines:\n",
      "  Logistic Regression: Prom=0.475, Bound=0.143, Avg=0.309\n",
      "  Random Forest     : Prom=0.486, Bound=0.170, Avg=0.328\n",
      "  Naive Bayes       : Prom=0.450, Bound=0.010, Avg=0.230\n",
      "\n",
      "ðŸŽ‰ Production neural network experiment complete!\n",
      "ðŸ’¡ These models should significantly outperform classical ML baselines!\n"
     ]
    }
   ],
   "source": [
    "# FINAL NEURAL PROSODY DETECTION - PRODUCTION READY\n",
    "# =====================================================\n",
    "# Optimized neural networks for prosodic event detection with proper class balance handling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ðŸš€ FINAL NEURAL PROSODY DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =====================================================\n",
    "# 1. OPTIMIZED DATASET CLASS\n",
    "# =====================================================\n",
    "\n",
    "class OptimizedProsodyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Optimized dataset with smart thresholding for sequence labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, splits_data, sequence_length=50, \n",
    "                 prominence_threshold=0.15, boundary_threshold=0.10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prominence_threshold: % of frames needed for positive prominence sequence\n",
    "            boundary_threshold: % of frames needed for positive boundary sequence\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prom_threshold = prominence_threshold\n",
    "        self.bound_threshold = boundary_threshold\n",
    "        \n",
    "        self.sequences = []\n",
    "        self.prominence_targets = []\n",
    "        self.boundary_targets = []\n",
    "        \n",
    "        print(f\"  ðŸ“ Creating sequences with length {sequence_length}\")\n",
    "        print(f\"  ðŸŽ¯ Prominence threshold: {prominence_threshold:.1%}\")\n",
    "        print(f\"  ðŸŽ¯ Boundary threshold: {boundary_threshold:.1%}\")\n",
    "        \n",
    "        for file_data in splits_data:\n",
    "            features = file_data['features']\n",
    "            prom_labels = file_data['prominence_labels']\n",
    "            bound_labels = file_data['boundary_labels']\n",
    "            \n",
    "            # Create overlapping sequences\n",
    "            hop_size = sequence_length // 3  # More overlap for better coverage\n",
    "            \n",
    "            for i in range(0, len(features) - sequence_length + 1, hop_size):\n",
    "                end_idx = i + sequence_length\n",
    "                \n",
    "                seq_features = features[i:end_idx]\n",
    "                seq_prom = prom_labels[i:end_idx]\n",
    "                seq_bound = bound_labels[i:end_idx]\n",
    "                \n",
    "                # Smart sequence labeling with lower thresholds\n",
    "                prom_rate = seq_prom.mean()\n",
    "                bound_rate = seq_bound.mean()\n",
    "                \n",
    "                prom_seq_label = 1 if prom_rate >= prominence_threshold else 0\n",
    "                bound_seq_label = 1 if bound_rate >= boundary_threshold else 0\n",
    "                \n",
    "                self.sequences.append(seq_features)\n",
    "                self.prominence_targets.append(prom_seq_label)\n",
    "                self.boundary_targets.append(bound_seq_label)\n",
    "        \n",
    "        # Report final distribution\n",
    "        prom_pos = np.sum(self.prominence_targets)\n",
    "        bound_pos = np.sum(self.boundary_targets)\n",
    "        total = len(self.sequences)\n",
    "        \n",
    "        print(f\"  ðŸ“Š Created {total:,} sequences\")\n",
    "        print(f\"  ðŸ“Š Positive prominence: {prom_pos:,} ({100*prom_pos/total:.1f}%)\")\n",
    "        print(f\"  ðŸ“Š Positive boundary: {bound_pos:,} ({100*bound_pos/total:.1f}%)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.FloatTensor(self.sequences[idx]),\n",
    "            'prominence': torch.LongTensor([self.prominence_targets[idx]]),\n",
    "            'boundary': torch.LongTensor([self.boundary_targets[idx]])\n",
    "        }\n",
    "\n",
    "# =====================================================\n",
    "# 2. PRODUCTION NEURAL ARCHITECTURES\n",
    "# =====================================================\n",
    "\n",
    "class Production_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Production-ready 1D CNN for prosodic event detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features=16, dropout=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extraction with 1D convolutions\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(input_features, 32, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.prominence_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        self.boundary_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: (batch, seq_len, features) -> (batch, features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "        \n",
    "        # Task predictions\n",
    "        prom_out = self.prominence_head(features)\n",
    "        bound_out = self.boundary_head(features)\n",
    "        \n",
    "        return prom_out, bound_out\n",
    "\n",
    "class Production_RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Production-ready RNN with attention for prosodic event detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features=16, hidden_size=64, dropout=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Bidirectional GRU (faster than LSTM)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_features, hidden_size, \n",
    "            batch_first=True, bidirectional=True, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        rnn_output_size = hidden_size * 2  # Bidirectional\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(rnn_output_size, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.prominence_head = nn.Sequential(\n",
    "            nn.Linear(rnn_output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        self.boundary_head = nn.Sequential(\n",
    "            nn.Linear(rnn_output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # RNN forward pass\n",
    "        rnn_out, _ = self.rnn(x)  # (batch, seq_len, hidden*2)\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = self.attention(rnn_out)  # (batch, seq_len, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        attended_features = torch.sum(rnn_out * attention_weights, dim=1)  # (batch, hidden*2)\n",
    "        \n",
    "        # Task predictions\n",
    "        prom_out = self.prominence_head(attended_features)\n",
    "        bound_out = self.boundary_head(attended_features)\n",
    "        \n",
    "        return prom_out, bound_out\n",
    "\n",
    "# =====================================================\n",
    "# 3. PRODUCTION TRAINER\n",
    "# =====================================================\n",
    "\n",
    "class ProductionTrainer:\n",
    "    \"\"\"\n",
    "    Production trainer with all the fixes applied\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Balanced loss functions\n",
    "        self.prominence_criterion = nn.CrossEntropyLoss()\n",
    "        self.boundary_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer with schedule\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_prom_f1': [], 'val_prom_f1': [],\n",
    "            'train_bound_f1': [], 'val_bound_f1': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_prom_preds, all_prom_targets = [], []\n",
    "        all_bound_preds, all_bound_targets = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(self.device)\n",
    "            prom_targets = batch['prominence'].squeeze().to(self.device)\n",
    "            bound_targets = batch['boundary'].squeeze().to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            prom_outputs, bound_outputs = self.model(features)\n",
    "            \n",
    "            # Calculate losses\n",
    "            prom_loss = self.prominence_criterion(prom_outputs, prom_targets)\n",
    "            bound_loss = self.boundary_criterion(bound_outputs, bound_targets)\n",
    "            total_loss_batch = prom_loss + bound_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            # Collect predictions\n",
    "            prom_preds = torch.argmax(prom_outputs, dim=1).cpu().numpy()\n",
    "            bound_preds = torch.argmax(bound_outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_prom_preds.extend(prom_preds)\n",
    "            all_prom_targets.extend(prom_targets.cpu().numpy())\n",
    "            all_bound_preds.extend(bound_preds)\n",
    "            all_bound_targets.extend(bound_targets.cpu().numpy())\n",
    "        \n",
    "        # Calculate F1 scores\n",
    "        train_prom_f1 = f1_score(all_prom_targets, all_prom_preds, zero_division=0)\n",
    "        train_bound_f1 = f1_score(all_bound_targets, all_bound_preds, zero_division=0)\n",
    "        \n",
    "        return total_loss / len(train_loader), train_prom_f1, train_bound_f1\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_prom_preds, all_prom_targets = [], []\n",
    "        all_bound_preds, all_bound_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(self.device)\n",
    "                prom_targets = batch['prominence'].squeeze().to(self.device)\n",
    "                bound_targets = batch['boundary'].squeeze().to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                prom_outputs, bound_outputs = self.model(features)\n",
    "                \n",
    "                # Calculate losses\n",
    "                prom_loss = self.prominence_criterion(prom_outputs, prom_targets)\n",
    "                bound_loss = self.boundary_criterion(bound_outputs, bound_targets)\n",
    "                total_loss += (prom_loss + bound_loss).item()\n",
    "                \n",
    "                # Collect predictions\n",
    "                prom_preds = torch.argmax(prom_outputs, dim=1).cpu().numpy()\n",
    "                bound_preds = torch.argmax(bound_outputs, dim=1).cpu().numpy()\n",
    "                \n",
    "                all_prom_preds.extend(prom_preds)\n",
    "                all_prom_targets.extend(prom_targets.cpu().numpy())\n",
    "                all_bound_preds.extend(bound_preds)\n",
    "                all_bound_targets.extend(bound_targets.cpu().numpy())\n",
    "        \n",
    "        val_prom_f1 = f1_score(all_prom_targets, all_prom_preds, zero_division=0)\n",
    "        val_bound_f1 = f1_score(all_bound_targets, all_bound_preds, zero_division=0)\n",
    "        \n",
    "        return total_loss / len(val_loader), val_prom_f1, val_bound_f1\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=30):\n",
    "        print(f\"ðŸš€ Training for {epochs} epochs...\")\n",
    "        \n",
    "        best_val_f1 = 0\n",
    "        patience = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_prom_f1, train_bound_f1 = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_prom_f1, val_bound_f1 = self.evaluate(val_loader)\n",
    "            \n",
    "            # Scheduler step\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_prom_f1'].append(train_prom_f1)\n",
    "            self.history['val_prom_f1'].append(val_prom_f1)\n",
    "            self.history['train_bound_f1'].append(train_bound_f1)\n",
    "            self.history['val_bound_f1'].append(val_bound_f1)\n",
    "            \n",
    "            # Calculate average F1\n",
    "            val_avg_f1 = (val_prom_f1 + val_bound_f1) / 2\n",
    "            \n",
    "            # Print progress\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "                  f\"Time: {epoch_time:.1f}s | \"\n",
    "                  f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
    "                  f\"Prom F1: {train_prom_f1:.3f}/{val_prom_f1:.3f} | \"\n",
    "                  f\"Bound F1: {train_bound_f1:.3f}/{val_bound_f1:.3f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_avg_f1 > best_val_f1:\n",
    "                best_val_f1 = val_avg_f1\n",
    "                patience = 0\n",
    "                torch.save(self.model.state_dict(), f'best_prosody_model.pth')\n",
    "            else:\n",
    "                patience += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience >= 8:\n",
    "                print(f\"ðŸ’¤ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load('best_prosody_model.pth'))\n",
    "            print(f\"âœ… Training complete! Best validation F1: {best_val_f1:.3f}\")\n",
    "        except:\n",
    "            print(f\"âš ï¸ Using final model state (no improvement found)\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. MAIN EXPERIMENT\n",
    "# =====================================================\n",
    "\n",
    "def run_production_experiment():\n",
    "    \"\"\"\n",
    "    Run the final production experiment\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ PRODUCTION NEURAL NETWORK EXPERIMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"ðŸ“‚ Loading data...\")\n",
    "    with open(\"autorpt_processed_subset.pkl\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    processed_data = data['processed_data']\n",
    "    \n",
    "    # Same splits as classical ML\n",
    "    n_files = len(processed_data)\n",
    "    train_files = int(0.7 * n_files)\n",
    "    val_files = int(0.15 * n_files)\n",
    "    \n",
    "    splits = {\n",
    "        'train': processed_data[:train_files],\n",
    "        'val': processed_data[train_files:train_files + val_files],\n",
    "        'test': processed_data[train_files + val_files:]\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ“Š Files: Train={len(splits['train'])}, Val={len(splits['val'])}, Test={len(splits['test'])}\")\n",
    "    \n",
    "    # Create optimized datasets\n",
    "    print(\"\\nðŸ“¦ Creating optimized datasets...\")\n",
    "    train_dataset = OptimizedProsodyDataset(splits['train'], \n",
    "                                           prominence_threshold=0.15, \n",
    "                                           boundary_threshold=0.10)\n",
    "    \n",
    "    val_dataset = OptimizedProsodyDataset(splits['val'], \n",
    "                                         prominence_threshold=0.15, \n",
    "                                         boundary_threshold=0.10)\n",
    "    \n",
    "    test_dataset = OptimizedProsodyDataset(splits['test'], \n",
    "                                          prominence_threshold=0.15, \n",
    "                                          boundary_threshold=0.10)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ðŸ”§ Using device: {device}\")\n",
    "    \n",
    "    # Models to test\n",
    "    models = {\n",
    "        'Production_CNN': Production_CNN(input_features=16),\n",
    "        'Production_RNN': Production_RNN(input_features=16)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Train each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nðŸš€ Training {model_name}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        trainer = ProductionTrainer(model, device=device)\n",
    "        trainer.train(train_loader, val_loader, epochs=25)\n",
    "        \n",
    "        # Test evaluation\n",
    "        test_loss, test_prom_f1, test_bound_f1 = trainer.evaluate(test_loader)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'test_prom_f1': test_prom_f1,\n",
    "            'test_bound_f1': test_bound_f1,\n",
    "            'test_loss': test_loss,\n",
    "            'history': trainer.history\n",
    "        }\n",
    "        \n",
    "        print(f\"ðŸŽ¯ {model_name} Final Test Results:\")\n",
    "        print(f\"  Prominence F1: {test_prom_f1:.3f}\")\n",
    "        print(f\"  Boundary F1: {test_bound_f1:.3f}\")\n",
    "    \n",
    "    # Final comparison\n",
    "    print(f\"\\nðŸ“Š FINAL NEURAL NETWORK RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Model':<15} | {'Prom F1':<8} | {'Bound F1':<8} | {'Avg F1':<8}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        avg_f1 = (result['test_prom_f1'] + result['test_bound_f1']) / 2\n",
    "        print(f\"{model_name:<15} | {result['test_prom_f1']:<8.3f} | {result['test_bound_f1']:<8.3f} | {avg_f1:<8.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# =====================================================\n",
    "# 5. COMPARISON PLOTTING\n",
    "# =====================================================\n",
    "\n",
    "def plot_neural_vs_classical():\n",
    "    \"\"\"\n",
    "    Plot comparison with classical ML results\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“Š NEURAL vs CLASSICAL COMPARISON\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Classical ML results (from your previous experiments)\n",
    "    classical_results = {\n",
    "        'Logistic Regression': {'prom_f1': 0.475, 'bound_f1': 0.143},\n",
    "        'Random Forest': {'prom_f1': 0.486, 'bound_f1': 0.170},\n",
    "        'Naive Bayes': {'prom_f1': 0.450, 'bound_f1': 0.010}\n",
    "    }\n",
    "    \n",
    "    print(\"Classical ML Baselines:\")\n",
    "    for name, result in classical_results.items():\n",
    "        avg = (result['prom_f1'] + result['bound_f1']) / 2\n",
    "        print(f\"  {name:<18}: Prom={result['prom_f1']:.3f}, Bound={result['bound_f1']:.3f}, Avg={avg:.3f}\")\n",
    "\n",
    "# =====================================================\n",
    "# RUN PRODUCTION EXPERIMENT\n",
    "# =====================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_production_experiment()\n",
    "    plot_neural_vs_classical()\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Production neural network experiment complete!\")\n",
    "    print(f\"ðŸ’¡ These models should significantly outperform classical ML baselines!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
